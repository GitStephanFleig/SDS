{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Exercise Set 12: Linear regression models.\n",
    "\n",
    "*Afternoon, August 19, 2019*\n",
    "\n",
    "In this Exercise Set 12 we will work with linear regression models.\n",
    "\n",
    "We import our standard stuff. Notice that we are not interested in seeing the convergence warning in scikit-learn so we suppress them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 12.1: Estimating linear models with gradient decent\n",
    " \n",
    "Normally we use OLS to estimate linear models. In this exercise we replace the OLS-estimator with a new estimator that we code up from scratch. We solve the numerical optimization using the gradient decent algorithm. Using our algorithm we will fit it to some data, and compare our own solution to the standard solution from `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.0**: Import the dataset `tips` from the `seaborn`.\n",
    "\n",
    "\n",
    "*Hint*: use the `load_dataset` method in seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25.29</td>\n",
       "      <td>4.71</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.77</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26.88</td>\n",
       "      <td>3.12</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15.04</td>\n",
       "      <td>1.96</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.78</td>\n",
       "      <td>3.23</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.27</td>\n",
       "      <td>1.71</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35.26</td>\n",
       "      <td>5.00</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15.42</td>\n",
       "      <td>1.57</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18.43</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.83</td>\n",
       "      <td>3.02</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21.58</td>\n",
       "      <td>3.92</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.33</td>\n",
       "      <td>1.67</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16.29</td>\n",
       "      <td>3.71</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>16.97</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.65</td>\n",
       "      <td>3.35</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17.92</td>\n",
       "      <td>4.08</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20.29</td>\n",
       "      <td>2.75</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.77</td>\n",
       "      <td>2.23</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>39.42</td>\n",
       "      <td>7.58</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19.82</td>\n",
       "      <td>3.18</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.81</td>\n",
       "      <td>2.34</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13.37</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>12.69</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>21.70</td>\n",
       "      <td>4.30</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>19.65</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>28.17</td>\n",
       "      <td>6.50</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>12.90</td>\n",
       "      <td>1.10</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>28.15</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>11.59</td>\n",
       "      <td>1.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>7.74</td>\n",
       "      <td>1.44</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>30.14</td>\n",
       "      <td>3.09</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>12.16</td>\n",
       "      <td>2.20</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>13.42</td>\n",
       "      <td>3.48</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>8.58</td>\n",
       "      <td>1.92</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>15.98</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>13.42</td>\n",
       "      <td>1.58</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>16.27</td>\n",
       "      <td>2.50</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>10.09</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>20.45</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>13.28</td>\n",
       "      <td>2.72</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>22.12</td>\n",
       "      <td>2.88</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>24.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>15.69</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>11.61</td>\n",
       "      <td>3.39</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>10.77</td>\n",
       "      <td>1.47</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>15.53</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>10.07</td>\n",
       "      <td>1.25</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>12.60</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>32.83</td>\n",
       "      <td>1.17</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>35.83</td>\n",
       "      <td>4.67</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>29.03</td>\n",
       "      <td>5.92</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>27.18</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>22.67</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>17.82</td>\n",
       "      <td>1.75</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>18.78</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Thur</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     total_bill   tip     sex smoker   day    time  size\n",
       "0         16.99  1.01  Female     No   Sun  Dinner     2\n",
       "1         10.34  1.66    Male     No   Sun  Dinner     3\n",
       "2         21.01  3.50    Male     No   Sun  Dinner     3\n",
       "3         23.68  3.31    Male     No   Sun  Dinner     2\n",
       "4         24.59  3.61  Female     No   Sun  Dinner     4\n",
       "5         25.29  4.71    Male     No   Sun  Dinner     4\n",
       "6          8.77  2.00    Male     No   Sun  Dinner     2\n",
       "7         26.88  3.12    Male     No   Sun  Dinner     4\n",
       "8         15.04  1.96    Male     No   Sun  Dinner     2\n",
       "9         14.78  3.23    Male     No   Sun  Dinner     2\n",
       "10        10.27  1.71    Male     No   Sun  Dinner     2\n",
       "11        35.26  5.00  Female     No   Sun  Dinner     4\n",
       "12        15.42  1.57    Male     No   Sun  Dinner     2\n",
       "13        18.43  3.00    Male     No   Sun  Dinner     4\n",
       "14        14.83  3.02  Female     No   Sun  Dinner     2\n",
       "15        21.58  3.92    Male     No   Sun  Dinner     2\n",
       "16        10.33  1.67  Female     No   Sun  Dinner     3\n",
       "17        16.29  3.71    Male     No   Sun  Dinner     3\n",
       "18        16.97  3.50  Female     No   Sun  Dinner     3\n",
       "19        20.65  3.35    Male     No   Sat  Dinner     3\n",
       "20        17.92  4.08    Male     No   Sat  Dinner     2\n",
       "21        20.29  2.75  Female     No   Sat  Dinner     2\n",
       "22        15.77  2.23  Female     No   Sat  Dinner     2\n",
       "23        39.42  7.58    Male     No   Sat  Dinner     4\n",
       "24        19.82  3.18    Male     No   Sat  Dinner     2\n",
       "25        17.81  2.34    Male     No   Sat  Dinner     4\n",
       "26        13.37  2.00    Male     No   Sat  Dinner     2\n",
       "27        12.69  2.00    Male     No   Sat  Dinner     2\n",
       "28        21.70  4.30    Male     No   Sat  Dinner     2\n",
       "29        19.65  3.00  Female     No   Sat  Dinner     2\n",
       "..          ...   ...     ...    ...   ...     ...   ...\n",
       "214       28.17  6.50  Female    Yes   Sat  Dinner     3\n",
       "215       12.90  1.10  Female    Yes   Sat  Dinner     2\n",
       "216       28.15  3.00    Male    Yes   Sat  Dinner     5\n",
       "217       11.59  1.50    Male    Yes   Sat  Dinner     2\n",
       "218        7.74  1.44    Male    Yes   Sat  Dinner     2\n",
       "219       30.14  3.09  Female    Yes   Sat  Dinner     4\n",
       "220       12.16  2.20    Male    Yes   Fri   Lunch     2\n",
       "221       13.42  3.48  Female    Yes   Fri   Lunch     2\n",
       "222        8.58  1.92    Male    Yes   Fri   Lunch     1\n",
       "223       15.98  3.00  Female     No   Fri   Lunch     3\n",
       "224       13.42  1.58    Male    Yes   Fri   Lunch     2\n",
       "225       16.27  2.50  Female    Yes   Fri   Lunch     2\n",
       "226       10.09  2.00  Female    Yes   Fri   Lunch     2\n",
       "227       20.45  3.00    Male     No   Sat  Dinner     4\n",
       "228       13.28  2.72    Male     No   Sat  Dinner     2\n",
       "229       22.12  2.88  Female    Yes   Sat  Dinner     2\n",
       "230       24.01  2.00    Male    Yes   Sat  Dinner     4\n",
       "231       15.69  3.00    Male    Yes   Sat  Dinner     3\n",
       "232       11.61  3.39    Male     No   Sat  Dinner     2\n",
       "233       10.77  1.47    Male     No   Sat  Dinner     2\n",
       "234       15.53  3.00    Male    Yes   Sat  Dinner     2\n",
       "235       10.07  1.25    Male     No   Sat  Dinner     2\n",
       "236       12.60  1.00    Male    Yes   Sat  Dinner     2\n",
       "237       32.83  1.17    Male    Yes   Sat  Dinner     2\n",
       "238       35.83  4.67  Female     No   Sat  Dinner     3\n",
       "239       29.03  5.92    Male     No   Sat  Dinner     3\n",
       "240       27.18  2.00  Female    Yes   Sat  Dinner     2\n",
       "241       22.67  2.00    Male    Yes   Sat  Dinner     2\n",
       "242       17.82  1.75    Male     No   Sat  Dinner     2\n",
       "243       18.78  3.00  Female     No  Thur  Dinner     2\n",
       "\n",
       "[244 rows x 7 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Answer to Ex. 12.1.0]\n",
    "tips = sns.load_dataset('tips')\n",
    "tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.1**: Convert non-numeric variables to dummy variables for each category (remember to leave one column out for each catagorical variable, so you have a reference). Restructure the data so we get a dataset `y` containing the variable tip, and a dataset `X` containing the \n",
    "features. \n",
    "\n",
    ">> *Hint*: You might want to use the `get_dummies` method in pandas, with the `drop_first = True` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     total_bill  size  sex_Female  smoker_No  day_Fri  day_Sat  day_Sun  \\\n",
      "0         16.99     2           1          1        0        0        1   \n",
      "1         10.34     3           0          1        0        0        1   \n",
      "2         21.01     3           0          1        0        0        1   \n",
      "3         23.68     2           0          1        0        0        1   \n",
      "4         24.59     4           1          1        0        0        1   \n",
      "5         25.29     4           0          1        0        0        1   \n",
      "6          8.77     2           0          1        0        0        1   \n",
      "7         26.88     4           0          1        0        0        1   \n",
      "8         15.04     2           0          1        0        0        1   \n",
      "9         14.78     2           0          1        0        0        1   \n",
      "10        10.27     2           0          1        0        0        1   \n",
      "11        35.26     4           1          1        0        0        1   \n",
      "12        15.42     2           0          1        0        0        1   \n",
      "13        18.43     4           0          1        0        0        1   \n",
      "14        14.83     2           1          1        0        0        1   \n",
      "15        21.58     2           0          1        0        0        1   \n",
      "16        10.33     3           1          1        0        0        1   \n",
      "17        16.29     3           0          1        0        0        1   \n",
      "18        16.97     3           1          1        0        0        1   \n",
      "19        20.65     3           0          1        0        1        0   \n",
      "20        17.92     2           0          1        0        1        0   \n",
      "21        20.29     2           1          1        0        1        0   \n",
      "22        15.77     2           1          1        0        1        0   \n",
      "23        39.42     4           0          1        0        1        0   \n",
      "24        19.82     2           0          1        0        1        0   \n",
      "25        17.81     4           0          1        0        1        0   \n",
      "26        13.37     2           0          1        0        1        0   \n",
      "27        12.69     2           0          1        0        1        0   \n",
      "28        21.70     2           0          1        0        1        0   \n",
      "29        19.65     2           1          1        0        1        0   \n",
      "..          ...   ...         ...        ...      ...      ...      ...   \n",
      "214       28.17     3           1          0        0        1        0   \n",
      "215       12.90     2           1          0        0        1        0   \n",
      "216       28.15     5           0          0        0        1        0   \n",
      "217       11.59     2           0          0        0        1        0   \n",
      "218        7.74     2           0          0        0        1        0   \n",
      "219       30.14     4           1          0        0        1        0   \n",
      "220       12.16     2           0          0        1        0        0   \n",
      "221       13.42     2           1          0        1        0        0   \n",
      "222        8.58     1           0          0        1        0        0   \n",
      "223       15.98     3           1          1        1        0        0   \n",
      "224       13.42     2           0          0        1        0        0   \n",
      "225       16.27     2           1          0        1        0        0   \n",
      "226       10.09     2           1          0        1        0        0   \n",
      "227       20.45     4           0          1        0        1        0   \n",
      "228       13.28     2           0          1        0        1        0   \n",
      "229       22.12     2           1          0        0        1        0   \n",
      "230       24.01     4           0          0        0        1        0   \n",
      "231       15.69     3           0          0        0        1        0   \n",
      "232       11.61     2           0          1        0        1        0   \n",
      "233       10.77     2           0          1        0        1        0   \n",
      "234       15.53     2           0          0        0        1        0   \n",
      "235       10.07     2           0          1        0        1        0   \n",
      "236       12.60     2           0          0        0        1        0   \n",
      "237       32.83     2           0          0        0        1        0   \n",
      "238       35.83     3           1          1        0        1        0   \n",
      "239       29.03     3           0          1        0        1        0   \n",
      "240       27.18     2           1          0        0        1        0   \n",
      "241       22.67     2           0          0        0        1        0   \n",
      "242       17.82     2           0          1        0        1        0   \n",
      "243       18.78     2           1          1        0        0        0   \n",
      "\n",
      "     time_Dinner  \n",
      "0              1  \n",
      "1              1  \n",
      "2              1  \n",
      "3              1  \n",
      "4              1  \n",
      "5              1  \n",
      "6              1  \n",
      "7              1  \n",
      "8              1  \n",
      "9              1  \n",
      "10             1  \n",
      "11             1  \n",
      "12             1  \n",
      "13             1  \n",
      "14             1  \n",
      "15             1  \n",
      "16             1  \n",
      "17             1  \n",
      "18             1  \n",
      "19             1  \n",
      "20             1  \n",
      "21             1  \n",
      "22             1  \n",
      "23             1  \n",
      "24             1  \n",
      "25             1  \n",
      "26             1  \n",
      "27             1  \n",
      "28             1  \n",
      "29             1  \n",
      "..           ...  \n",
      "214            1  \n",
      "215            1  \n",
      "216            1  \n",
      "217            1  \n",
      "218            1  \n",
      "219            1  \n",
      "220            0  \n",
      "221            0  \n",
      "222            0  \n",
      "223            0  \n",
      "224            0  \n",
      "225            0  \n",
      "226            0  \n",
      "227            1  \n",
      "228            1  \n",
      "229            1  \n",
      "230            1  \n",
      "231            1  \n",
      "232            1  \n",
      "233            1  \n",
      "234            1  \n",
      "235            1  \n",
      "236            1  \n",
      "237            1  \n",
      "238            1  \n",
      "239            1  \n",
      "240            1  \n",
      "241            1  \n",
      "242            1  \n",
      "243            1  \n",
      "\n",
      "[244 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# [Answer to Ex. 12.1.1]\n",
    "dummy = pd.get_dummies(tips, columns=['sex','smoker','day','time'], drop_first=True)\n",
    "dummy\n",
    "\n",
    "y = dummy['tip']\n",
    "X = dummy.drop('tip',1) # 1 is the axis number for columns\n",
    "#print(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.2**: Divide the features and target into test and train data. Make the split 50 pct. of each. The split data should be called `X_train`, `X_test`, `y_train`, `y_test`.\n",
    "\n",
    ">> *Hint*: You may use `train_test_split` in `sklearn.model_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.2]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)\n",
    "#len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.3**: Normalize your features by converting to zero mean and one std. deviation.\n",
    "\n",
    ">> *Hint 1*: Take a look at `StandardScaler` in `sklearn.preprocessing`. \n",
    "\n",
    ">> *Hint 2*: If in doubt about which distribution to scale, you may read [this post](https://stats.stackexchange.com/questions/174823/how-to-apply-standardization-normalization-to-train-and-testset-if-prediction-i)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarah\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Sarah\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n",
      "C:\\Users\\Sarah\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# [Answer to Ex. 12.1.3]\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "norm_scaler = StandardScaler().fit(X_train) \n",
    "X_train = norm_scaler.transform(X_train) \n",
    "X_test = norm_scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.28200982,  0.37603995, -0.71145825,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.20397075,  1.37336328, -0.71145825, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.07018948, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-1.43475844, -1.61860673,  1.40556386,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.42916922, -0.62128339,  1.40556386, -1.35543694,  3.77491722,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-1.60198503, -0.62128339,  1.40556386, -1.35543694,  3.77491722,\n",
       "        -0.73776947, -0.62158156,  0.67259271],\n",
       "       [-0.24521997, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [ 0.35791059, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.21957856,  0.37603995,  1.40556386, -1.35543694, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [ 1.61991392,  1.37336328, -0.71145825, -1.35543694, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [ 0.57530516, -0.62128339,  1.40556386, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [ 0.56750125, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.67443489, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.79372319, -0.62128339,  1.40556386, -1.35543694, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-0.74689974, -0.62128339, -0.71145825, -1.35543694,  3.77491722,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [ 1.14610525,  1.37336328, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-1.31212561, -0.62128339,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-0.68223879, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.1493434 , -0.62128339,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947, -0.62158156,  0.67259271],\n",
       "       [ 0.33226918,  0.37603995, -0.71145825, -1.35543694, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [ 1.10820055,  0.37603995, -0.71145825, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [ 0.0892332 , -0.62128339,  1.40556386,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.45927001, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-0.56629502, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.73686615, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-0.41467625, -0.62128339,  1.40556386, -1.35543694, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-0.88737008, -0.62128339, -0.71145825, -1.35543694,  3.77491722,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [ 1.07921461,  3.36800995,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [ 0.57642   ,  1.37336328, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.48491142, -0.62128339,  1.40556386,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.60085519, -0.62128339,  1.40556386,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.85169507, -0.62128339,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-0.52393096, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-1.06240057, -0.62128339,  1.40556386, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.20397075, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.75024427, -0.62128339,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.35558952,  0.37603995,  1.40556386,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-1.09138651, -0.62128339,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [ 3.14502041,  1.37336328, -0.71145825,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.4614997 ,  0.37603995,  1.40556386,  0.73776947,  3.77491722,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-1.2864842 , -1.61860673, -0.71145825, -1.35543694,  3.77491722,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-0.85726929, -0.62128339,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-0.49383017,  0.37603995, -0.71145825, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [ 1.08478883,  2.37068662,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.82047944, -0.62128339,  1.40556386, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.35113015,  0.37603995,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.20954497, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [ 0.1628129 , -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.33998171,  0.37603995,  1.40556386,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [ 0.75368018,  1.37336328, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [ 1.40028966, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-0.24856451, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [ 0.04129492, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [ 3.1271829 ,  3.36800995, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-1.17277012, -0.62128339,  1.40556386, -1.35543694, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.90186305, -0.62128339, -0.71145825, -1.35543694,  3.77491722,\n",
       "        -0.73776947, -0.62158156,  0.67259271],\n",
       "       [ 1.09370758,  1.37336328, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [ 0.06359179,  1.37336328,  1.40556386,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.65325285, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-0.83162788, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [ 0.79827394, -0.62128339, -0.71145825, -1.35543694,  3.77491722,\n",
       "        -0.73776947, -0.62158156,  0.67259271],\n",
       "       [-0.48045204, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [ 0.11598946, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [ 0.54408953,  1.37336328,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [ 1.1171193 ,  1.37336328,  1.40556386, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.51166767, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.40687234,  0.37603995,  1.40556386, -1.35543694, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [ 1.41701232, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-1.09138651,  0.37603995,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [ 0.98668257, -0.62128339, -0.71145825, -1.35543694,  3.77491722,\n",
       "        -0.73776947, -0.62158156,  0.67259271],\n",
       "       [-0.52839033, -0.62128339,  1.40556386, -1.35543694,  3.77491722,\n",
       "        -0.73776947, -0.62158156,  0.67259271],\n",
       "       [-1.07131932, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [ 0.49838093,  1.37336328,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.64767863, -0.62128339,  1.40556386, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.70342083, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.16495121,  0.37603995,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [ 1.63775142,  1.37336328,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.40910203, -0.62128339,  1.40556386,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.75247396, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.20731528, -0.62128339,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-1.29428811, -0.62128339,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-0.95091618, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.45481063,  0.37603995, -0.71145825,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.18836293,  1.37336328, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [ 0.39693013, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.31879967,  0.37603995, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.03339963, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-1.17834434, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-1.38013109, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [ 0.01899804, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [ 1.99450147,  1.37336328, -0.71145825, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [ 1.61768423, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [ 0.89749505,  0.37603995,  1.40556386, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [ 0.05913242,  0.37603995, -0.71145825,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [ 2.81279692,  0.37603995, -0.71145825, -1.35543694, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [ 0.30105355,  0.37603995, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [ 0.60652079,  1.37336328, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [ 1.7514655 ,  0.37603995,  1.40556386,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.0757637 , -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-1.09584589, -0.62128339,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.46484423, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-0.82270913, -0.62128339,  1.40556386, -1.35543694, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-0.11589808, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-0.79037865, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-1.09807558, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.66551613, -0.62128339,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [ 1.30775762, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-1.29317326, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-0.29650279, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.58970675, -0.62128339,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.82828335, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.84500601, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [-0.74689974, -0.62128339,  1.40556386, -1.35543694,  3.77491722,\n",
       "        -0.73776947, -0.62158156, -1.48678388],\n",
       "       [-0.24521997, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.43585828,  0.37603995,  1.40556386,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [ 0.23527776, -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [ 0.0713957 , -0.62128339, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [ 0.99337163,  0.37603995, -0.71145825,  0.73776947, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [ 0.15389415,  1.37336328, -0.71145825,  0.73776947, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271],\n",
       "       [ 3.4215017 ,  0.37603995, -0.71145825, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-1.01446228, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "         1.35543694, -0.62158156,  0.67259271],\n",
       "       [-0.49383017, -0.62128339, -0.71145825, -1.35543694, -0.26490647,\n",
       "        -0.73776947,  1.60879933,  0.67259271]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.4**: Make a function called `compute_error` to compute the prediction errors given input target `y_`, input features `X_` and input weights `w_`. You should use matrix multiplication.\n",
    ">\n",
    ">> *Hint 1:* You can use the net-input fct. from yesterday.\n",
    ">>\n",
    ">> *Hint 2:* If you run the following code,\n",
    ">> ```python\n",
    "y__ = np.array([1,1])\n",
    "X__ = np.array([[1,0],[0,1]])\n",
    "w__ = np.array([0,1,1])\n",
    "compute_error(y__, X__, w__)\n",
    "```\n",
    "\n",
    ">> then you should get output:\n",
    "```python \n",
    "array([0,0])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "# [Answer to Ex. 12.1.4]\n",
    "\n",
    "def net_input(X,w):\n",
    "    z = w[0] + X.dot(w[1:]) # compute net-input\n",
    "    return z\n",
    "\n",
    "def predict(X,w):\n",
    "    positive = net_input(X,w)>=0 # computes prediction (boolean)\n",
    "    y_hat = np.where(positive, 1, -1) # converts prediction\n",
    "    return y_hat\n",
    "\n",
    "def compute_error(y, X, w):\n",
    "    return y - predict(X, w)\n",
    "\n",
    "    \n",
    "y__ = np.array([1,1])\n",
    "X__ = np.array([[1,0],[0,1]])\n",
    "w__ = np.array([0,1,1])\n",
    "print(compute_error(y__, X__, w__))\n",
    "\n",
    "#w_input = weights(0,0.01,117)\n",
    "#predict(X_train,w_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.5**: Make a function to update the weights given input target `y_`, input features `X_` and input weights `w_` as well as learning rate, $\\eta$, i.e. greek `eta`. You should use matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.5]\n",
    "\n",
    "def update_weights(X, y, w, eta):\n",
    "    #errors = 0\n",
    "    for xi,yi in zip(X,y):\n",
    "        update = eta * X.T.dot(compute_error(y, X, w))\n",
    "        w[1:] = w[1:] + update\n",
    "        w[0] = w[0] + eta*compute_error(y, X, w).sum()\n",
    "        #errors = errors + int(update != 0)\n",
    "    return w\n",
    "\n",
    "#update_weights(X_train,y_train,w_input,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.6**: Use the code below to initialize weights `w` at zero given feature set `X`. Notice how we include an extra weight that includes the bias term. Set the learning rate `eta` to 0.001. Make a loop with 50 iterations where you iteratively apply your weight updating function. \n",
    "\n",
    ">```python\n",
    "w = np.zeros(1+X.shape[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.6]\n",
    "\n",
    "w = np.zeros(1+X.shape[1])\n",
    "eta = 0.001\n",
    "\n",
    "for i in range(50):\n",
    "    update_weights(X_train, y_train, w, eta)\n",
    "\n",
    "#print(update_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.7**: Make a function to compute the mean squared error. Alter the loop so it makes 100 iterations and computes the MSE for test and train after each iteration, plot these in one figure. \n",
    "\n",
    ">> Hint: You can use the following code to check that your model works:\n",
    ">>```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "assert((w[1:] - reg.coef_).sum() < 0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.7]\n",
    "def MSE(y_, X_, w_):\n",
    "    error_squared = compute_error(y, X, w)**2\n",
    "    return error_squared.sum() / len(y_)\n",
    "\n",
    "w = np.zeros(X.shape[1]+1)\n",
    "\n",
    "MSE_train = [MSE(y_train, X_train, w)]\n",
    "MSE_test = [MSE(y_test, X_test, w)]\n",
    "\n",
    "for i in range(100):\n",
    "    update_weights(X, y, w, 10**-3)\n",
    "    \n",
    "    MSE_train.append(MSE(y_train, X_train, w))\n",
    "    MSE_test.append(MSE(y_test, X_test, w))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ac98582f28>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(MSE_train).plot()\n",
    "pd.Series(MSE_test).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following bonus exercises are for those who have completed all other exercises until now and have a deep motivation for learning more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.8 (BONUS)**: Implement your linear regression model as a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.9 (BONUS)**: Is it possible to adjust our linear model to become a Lasso? Is there a simple fix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 12.2: Houseprices\n",
    "In this example we will try to predict houseprices using a lot of variable (or features as they are called in Machine Learning). We are going to work with Kaggle's dataset on house prices, see information [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). Kaggle is an organization that hosts competitions in building predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.2.0:** Load the california housing data with scikit-learn using the code below. Inspect the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cal_house = fetch_california_housing()    \n",
    "X = pd.DataFrame(data=cal_house['data'], \n",
    "                 columns=cal_house['feature_names'])\\\n",
    "             .iloc[:,:-2]\n",
    "y = cal_house['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10320, 6)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Answer to Ex. 12.2.0]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **Ex.12.2.1**: Generate interactions between all features to third degree, make sure you **exclude** the bias/intercept term. How many variables are there? Will OLS fail? \n",
    "\n",
    "> After making interactions rescale the features to have zero mean, unit std. deviation. Should you use the distribution of the training data to rescale the test data?  \n",
    "\n",
    ">> *Hint 1*: Try importing `PolynomialFeatures` from `sklearn.preprocessing`\n",
    "\n",
    ">> *Hint 2*: If in doubt about which distribution to scale, you may read [this post](https://stats.stackexchange.com/questions/174823/how-to-apply-standardization-normalization-to-train-and-testset-if-prediction-i)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.2.1]\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# generate interactions between all features to third degree\n",
    "X_train_p = PolynomialFeatures(degree=3, include_bias=False).fit_transform(X_train)\n",
    "X_test_p = PolynomialFeatures(degree=3, include_bias=False).fit_transform(X_test)\n",
    "#print(X_train_p.shape)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# rescale features\n",
    "norm_scaler = StandardScaler().fit(X_train_p)\n",
    "X_train_s = norm_scaler.transform(X_train_p) # only use .fit in training data\n",
    "X_test_s = norm_scaler.transform(X_test_p)\n",
    "#print(X_train_s.shape)\n",
    "#print(X_test_s.shape)\n",
    "\n",
    "# we change the distribution of the training data and then \n",
    "# use that distribution to rescale the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.12.2.2**: Estimate the Lasso model on the train data set, using values of $\\lambda$ in the range from $10^{-4}$ to $10^4$. For each $\\lambda$  calculate and save the Root Mean Squared Error (RMSE) for the test and train data. \n",
    "\n",
    "> *Hint*: use `logspace` in numpy to create the range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.2.2]\n",
    "# Lasso regression helps reduce over-fitting and with feature selection\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10320, 3570)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PolynomialFeatures(include_bias=True).fit_transform(X_train_s).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10320, 83)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "0.000774263682681127\n",
      "0.005994842503189409\n",
      "0.046415888336127774\n",
      "0.3593813663804626\n",
      "2.782559402207126\n",
      "21.54434690031882\n",
      "166.81005372000558\n",
      "1291.5496650148827\n",
      "10000.0\n"
     ]
    }
   ],
   "source": [
    "#perform = []\n",
    "lambdas = np.logspace(-4, 4, 10)\n",
    "rmse_train = []\n",
    "rmse_test = []\n",
    "for lambda_ in lambdas:\n",
    "    print(lambda_)\n",
    "    lasso = Lasso(alpha=lambda_, random_state=1)\n",
    "    lasso.fit(X_train_s, y_train) # fits on the training  data\n",
    "    y_pred_train = lasso.predict(X_train_s) # predicts how it performs on the validation data\n",
    "    y_pred_test = lasso.predict(X_test_s)\n",
    "    rmse_train.append(sqrt(mse(y_pred_train, y_train)))\n",
    "    rmse_test.append(sqrt(mse(y_pred_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda</th>\n",
       "      <th>rmse_train</th>\n",
       "      <th>rmse_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.698086</td>\n",
       "      <td>8.369685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.700214</td>\n",
       "      <td>5.826745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005995</td>\n",
       "      <td>0.733008</td>\n",
       "      <td>1.404345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.046416</td>\n",
       "      <td>0.803397</td>\n",
       "      <td>0.811945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.359381</td>\n",
       "      <td>0.898689</td>\n",
       "      <td>0.901714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.782559</td>\n",
       "      <td>1.150274</td>\n",
       "      <td>1.157599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21.544347</td>\n",
       "      <td>1.150274</td>\n",
       "      <td>1.157599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>166.810054</td>\n",
       "      <td>1.150274</td>\n",
       "      <td>1.157599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1291.549665</td>\n",
       "      <td>1.150274</td>\n",
       "      <td>1.157599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>1.150274</td>\n",
       "      <td>1.157599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lambda  rmse_train  rmse_test\n",
       "0      0.000100    0.698086   8.369685\n",
       "1      0.000774    0.700214   5.826745\n",
       "2      0.005995    0.733008   1.404345\n",
       "3      0.046416    0.803397   0.811945\n",
       "4      0.359381    0.898689   0.901714\n",
       "5      2.782559    1.150274   1.157599\n",
       "6     21.544347    1.150274   1.157599\n",
       "7    166.810054    1.150274   1.157599\n",
       "8   1291.549665    1.150274   1.157599\n",
       "9  10000.000000    1.150274   1.157599"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparam_perform = pd.DataFrame({'lambda':lambdas,\n",
    "                                   'rmse_train':rmse_train,\n",
    "                                   'rmse_test':rmse_test})\n",
    "hyperparam_perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6535071814204865, 0.6593090656393251, 0.6693038367312137, 0.6833626244495304, 0.7050208469281987, 0.7395637687796323, 0.7839206717926924, 0.8081634529809827, 0.8467540437549516, 1.0362260118324655, 1.1502742926333995, 1.1502742926333995, 1.1502742926333995, 1.1502742926333995, 1.1502742926333995, 1.1502742926333995, 1.1502742926333995, 1.1502742926333995, 1.1502742926333995, 1.1502742926333995] \n",
      " [25.109466749889275, 33.67984455905152, 33.34469254965163, 28.379292650078188, 6.123666936264738, 0.7618739638389014, 0.7890051394132079, 0.8148527682362268, 0.8502175095991172, 1.0412001100155595, 1.1575985329014546, 1.1575985329014546, 1.1575985329014546, 1.1575985329014546, 1.1575985329014546, 1.1575985329014546, 1.1575985329014546, 1.1575985329014546, 1.1575985329014546, 1.1575985329014546]\n"
     ]
    }
   ],
   "source": [
    "print(rmse_train,'\\n',rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.12.2.3**: Make a plot with lambdas on the x-axis and the RMSE measures on the y-axis. What happens to RMSE for train and test data as $\\lambda$ increases? The x-axis should be log scaled. Which one are we interested in minimizing? \n",
    "\n",
    "> Bonus: Can you find the lambda that gives the lowest MSE-test score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ac9b8adbe0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEOCAYAAACKDawAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl0VfW9/vH3J3NIIAkQQEAmMTIEEiAgICqIkvSqnbW2thVt5VocblltbR07OfWWXlqL1lJba1scsf6s1lpHVKpVAgUZIiASFEQIU5gzfn9/nEMMZDoJZ2ef4XmtdVZy9tln74es5Mnmm72/25xziIhI7EvwO4CIiHQOFb6ISJxQ4YuIxAkVvohInFDhi4jECRW+iEicUOGLiMQJFb6ISJxQ4YuIxAkVvohInEjyO0BjPXv2dIMGDfI7hohIVFm2bNlO51xuW+tFVOEPGjSI0tJSv2OIiEQVM9scynoa0hERiRMqfBGROKHCFxGJExE1hi8iTdXU1LBlyxaOHDnidxTxWVpaGv379yc5OblD71fhi0S4LVu20LVrVwYNGoSZ+R1HfOKcY9euXWzZsoXBgwd3aBsa0hGJcEeOHKFHjx4q+zhnZvTo0eOE/qcXG4V/YAeUPeN3ChHPqOwFTvz7IDYK/+XbYNEVsO8jv5OIxKTExEQKCwvJz8/nwgsvZO/evQCUl5djZtxyyy0N6+7cuZPk5GSuueYaANatW8fUqVMpLCxk+PDhzJo1C4DFixeTlZVFYWFhw+PFF19ssu877rijQ5m/+c1vsnbt2lbXue+++/jTn/7Uoe23ZvHixVxwwQWtrrNixQqeffbZsO+7NbFR+Gd+B1w9vPZzv5OIxKT09HRWrFjB6tWr6d69O/fcc0/Da0OGDOGZZz75H/bjjz/OyJEjG55fd911zJkzhxUrVlBWVsa1117b8NqZZ57JihUrGh7nnntuk323VPjOOerr61vMfP/99zNixIhW/11XXXUVX//611tdxysq/I7KGQhjvw7L/wR7yv1OIxLTJk2axNatWxuep6enM3z48Iar5B999FEuvvjihte3bdtG//79G56PGjUq5H394Ac/4PDhwxQWFnLppZdSXl7O8OHDmT17NmPHjuXDDz/kW9/6FkVFRYwcOZIf/vCHDe+dOnVqQ6bMzExuuukmCgoKmDhxItu3bwfgRz/6EXPnzm1Y//vf/z4TJkwgLy+P119/HYBDhw5x8cUXM3r0aL70pS9x+umnNzsjwHPPPcewYcOYMmUKf/3rXxuWv/3220yePJkxY8YwefJk1q1bR3V1NbfeeiuPPvoohYWFPProo82uF26xc5bOWd+F//wFXv1f+Oy9fqcR8cSPn17D2o/2hXWbI/p244cXjmx7RaCuro6XXnqJb3zjG8csv+SSS3jkkUfo06cPiYmJ9O3bl48+Cgyxzpkzh3POOYfJkyczY8YMLr/8crKzswF4/fXXKSwsbNjOE088wSmnnNLw/K677mL+/PmsWLECCAwhrVu3jgceeIB77w38nN9+++10796duro6pk+fzjvvvMPo0aOPyXfw4EEmTpzI7bffzvXXX8/vfvc7br755ib/vtraWt5++22effZZfvzjH/Piiy9y7733kpOTwzvvvMPq1auPyXvUkSNHuPLKK3n55ZcZOnQoX/rSlxpeGzZsGK+99hpJSUm8+OKL3HjjjTzxxBP85Cc/obS0lPnz5wOwb9++ZtcLp9g4wgfo1hfGfxNWPgw7N/idRiSmHD3K7tGjB7t37+a888475vWSkhJeeOEFHn744WPKDuDyyy+nrKyMiy66iMWLFzNx4kSqqqqApkM6jcu+JQMHDmTixIkNzx977DHGjh3LmDFjWLNmTbPj9ikpKQ1j6uPGjaO8vLzZbX/+859vss6SJUu45JJLAMjPz2/yywTg3XffZfDgwZx66qmYGV/96lcbXqusrOSiiy4iPz+fOXPmsGbNmmb3Hep6JyJ2jvABpsyBZQ/A4jvhi3/wO41I2IV6JB5uR8fwKysrueCCC7jnnnu47rrrGl5PSUlh3Lhx/OIXv2DNmjU8/fTTx7y/b9++XHHFFVxxxRXk5+ezevXqDmfJyMho+HzTpk3MnTuXpUuXkpOTw8yZM5s9bTE5ObnhDJfExERqa2ub3XZqamqTdZxzIeVq6QyaW265hWnTpvHkk09SXl7O1KlTT2i9E+HpEb6ZzTGzNWa22sweNrM0L/dHZi6cfhWsfgI+7vg3lIg0Lysri7vvvpu5c+dSU1NzzGvf+c53+NnPfkaPHj2OWf7cc881rPvxxx+za9cu+vXrF/I+k5OTm+zrqH379pGRkUFWVhbbt2/nH//4Rzv/RW2bMmUKjz32GABr165l1apVTdYZNmwYmzZtYuPGjQA8/PDDDa9VVlY2/Hv/+Mc/Nizv2rUr+/fvb3O9cPKs8M2sH3AdUOScywcSgUu82l+DyddCarfAUb6IhN2YMWMoKCjgkUceOWb5yJEjueyyy5qs//zzz5Ofn09BQQHFxcX8/Oc/p0+fPsAnY/hHH4sWLWry/lmzZjF69GguvfTSJq8VFBQwZswYRo4cyRVXXMEZZ5wRpn/lJ2bPnk1FRQWjR4/mZz/7GaNHjyYrK+uYddLS0liwYAHnn38+U6ZMYeDAgQ2vXX/99dxwww2cccYZ1NXVNSyfNm0aa9eubfijbUvrhZOF+t+Vdm84UPj/BgqAfcD/A+52zj3f0nuKiopcWObDf/V/4ZXb4cpXoN/YE9+eiI/KysoYPny43zHiVl1dHTU1NaSlpbFx40amT5/O+vXrSUlJ8SVPc98PZrbMOVfU1ns9O8J3zm0F5gIfANuAytbKPqxOvwrSuwcuyBIROQGHDh1iypQpFBQU8LnPfY7f/OY3vpX9ifLsj7ZmlgN8BhgM7AUeN7OvOuf+ctx6s4BZAAMGDAjPztO6wZRvwwu3wuY3YODk8GxXROJO165dY+ZOfF7+0fZcYJNzrsI5VwP8FWjSvM65Bc65IudcUW5um7dkDN34KyGjV+Ao36NhKxGRaOJl4X8ATDSzLhY4X2k6UObh/o6V0iVwMdbmf8H7iztttyIikcrLMfy3gEXAcmBVcF8LvNpfs8bNhG79dZQvIoLH5+E7537onBvmnMt3zn3NOVfl5f6aSEqFs78HW0th/T87ddciIpEmdqZWaEnhpZAzGF65DVqZWU9EWhaN0yND4AKmo3P6tKa8vJz8/Pw213nooYc6nCUSxH7hJybD1Bvg41VQ9je/04hEpUicHjkUoRZ+KFT40WLUF6HnafDKHVDvzRVsIvHCz+mRAf7yl78wYcIECgsL+e///m/q6uqoq6tj5syZ5OfnM2rUKObNm8eiRYsoLS3l0ksvpbCwkMOHDx+z7WXLllFQUMCkSZOO+QVWXl7OmWeeydixYxk7dixvvPFGQ5ajVwbPmzevxfUiWWxNntaShESYdiM8fhmsehwKvJ/hQcQT//hB4H+r4dRnFHzqrpBW9Xt65LKyMh599FH+9a9/kZyczOzZs1m4cCEjR45k69atDZOy7d27l+zsbObPn8/cuXMpKmp6Eerll1/Or3/9a84++2y+973vNSzv1asXL7zwAmlpaWzYsIEvf/nLlJaWctdddzF37tyG/80cOnSo2fUiWXwUPsDwTwe+sRffCflfCAz1iEhIjh5ll5eXM27cuGanR77lllvo3bt3s9MjFxcX89xzz/HUU0/x29/+lpUrVwKBIZ3Gw0Fteemll1i2bBnjx49vyNWrVy8uvPBC3n//fa699lrOP/98ZsyY0ep2Kisr2bt3L2effTYAX/va1xomXqupqeGaa65hxYoVJCYmsn79+ma3Eep6kSR+Cj8hAabdDA9/CVYsDJyyKRJtQjwSD7dImR7ZOcdll13GnXc2nRxx5cqV/POf/+See+7hscce4w9/aHmKdOdci9MZz5s3j969e7Ny5Urq6+tJS2t+kt9Q14sk8TGGf1ReMfQrCkyuVtN0zmwRaZ3f0yNPnz6dRYsWsWPHDgB2797N5s2b2blzJ/X19XzhC1/gpz/9KcuXLweaTkF8VHZ2NllZWSxZsgSAhQsXNrxWWVnJSSedREJCAn/+858bZq5sbjrj5taLZPFV+GZwzs2wbyssf9DvNCJRyc/pkUeMGMFtt93GjBkzGD16NOeddx7btm1j69atDad+zpw5s+F/ADNnzuSqq65q9o+2DzzwAFdffTWTJk0iPT29Yfns2bN58MEHmThxIuvXr2+44cro0aNJSkqioKCAefPmtbheJPNseuSOCNv0yK1xDh68ECrWwf+sDEzBIBLBND2yNBaR0yNHLDOYdhMc3AFLf+d3GhGRThN/hQ8wcBIMPReW/BKO7PM7jYhIp4jPwofAUf7h3fDWfX4nERHpFPFb+P3GwrAL4I1fw6HdfqcRaVUk/a1N/HOi3wfxW/gQuPq2an+g9EUiVFpaGrt27VLpxznnHLt27Tqh8/3j58Kr5vQeCfmfDwzrTJwNmWG845ZImPTv358tW7ZQUVHhdxTxWVpa2jHzErVXfBc+wNQbYc2TsGQelHR8Vj4RryQnJzN48GC/Y0gMiO8hHYCeQ6HgK7D0ftgXnmlURUQikWeFb2anmdmKRo99ZvZtr/Z3Qs6+Hlw9vDbX7yQiIp7x8p6265xzhc65QmAccAh40qv9nZCcgTD267D8T7Cn3O80IiKe6KwhnenARufc5k7aX/ud9V2wBHj1534nERHxRGcV/iXAw520r47p1hfGfxNWPgQ7N/idRkQk7DwvfDNLAT4NPN7C67PMrNTMSn0/7WzKHEhKD9wkRUQkxnTGEf6ngOXOue3NveicW+CcK3LOFeXm+nwefGYuTLwKVj8BH3fsBg0iIpGqMwr/y0T6cE5jk6+F1Cwd5YtIzPG08M2sC3Ae8Fcv9xNW6Tkw+Rp49xnYutzvNCIiYeNp4TvnDjnnejjnKr3cT9idfhWkd4dXbvc7iYhI2OhK2+akdYMp34b3XoTNb/qdRkQkLFT4LRl/JWT0gpdvC9wWUUQkyqnwW5LSJXAx1uYlsOlVv9OIiJwwFX5rxs2Ebv3hpZ/qKF9Eop4KvzVJqYGJ1baWwvp/+p1GROSEqPDbUvgVyBkMr9wG9fV+pxER6TAVflsSk2HqDfDxKij7m99pREQ6TIUfilFfhJ6nwSt3QH2d32lERDpEhR+KhMTADc93roNVi/xOIyLSISr8UA3/NPQZFZhjp67G7zQiIu2mwg9VQgJMuxn2bIIVD/mdRkSk3VT47ZFXDP3Hw6v/C7VVfqcREWkXFX57mME5N8O+LbDsj36nERFpFxV+ew0+GwadCa/NhepDfqcREQmZCr+9zGDaTXBwByz9nd9pRERCpsLviIGTYOi5sOSXcGSf32lEREKiwu+oaTfB4d3w1n1+JxERCYnXtzjMNrNFZvaumZWZ2SQv99ep+o2FYRfAG7+GQ7v9TiMi0iavj/B/BTznnBsGFABlHu+vc027Ear2w5vz/U4iItImzwrfzLoBZwG/B3DOVTvn9nq1P1/0Hgn5X4B/3wcHKvxOIyLSKi+P8IcAFcADZvYfM7vfzDI83J8/pt4AtYdhyTy/k4iItMrLwk8CxgK/cc6NAQ4CPzh+JTObZWalZlZaURGFR8k9h0LBV2Dp/bDvI7/TiIi0yMvC3wJscc69FXy+iMAvgGM45xY454qcc0W5ubkexvHQ2deDqw9cjCUiEqE8K3zn3MfAh2Z2WnDRdGCtV/vzVc5AGPt1WP4n2FPudxoRkWZ5fZbOtcBCM3sHKATu8Hh//jnru2AJOsoXkYjlaeE751YEh2tGO+c+65zb4+X+fNWtL4y6CNY+pfnyRSQi6UrbcDrtU1C1Dz540+8kIiJNqPDDachUSEyB9f/0O4mISBMq/HBKzQxMnbz+Ob+TiIg0ocIPt7wS2PUe7HzP7yQiIsdQ4Ydb3ozAxw0a1hGRyKLCD7ecQZA7XMM6IhJxVPheyCuGzW/AkUq/k4iINFDheyGvBOprYePLficREWmgwvdC//GQnqPTM0UkorRZ+GaWGsoyaSQxCYaeBxueh/o6v9OIiAChHeE3d9moLiVtS14xHNoFW5f5nUREBAjMWd8sM+sD9APSzWwMYMGXugFdOiFbdBs6HSwxcLbOyRP8TiMi0nLhA8XATKA/8As+Kfz9wI3exooB6TkwYFJgHH/6rX6nERFpufCdcw8CD5rZF5xzT3RiptiRVwwv3AJ7P4Tsk/1OIyJxLpQx/P5m1s0C7jez5WY2w/NksSCvJPBRV92KSAQIpfCvcM7tA2YAvYDLgbs8TRUrep4KOYN1eqaIRITWxvCPOjp2/1/AA865lWZmrb2h4Y1m5QTG/OuAWudcUYdSRiuzwFF+6R+g+iCkZPidSETiWChH+MvM7HkChf9PM+sK1LdjH9Occ4VxV/ZH5RVDXRVses3vJCIS50Ip/G8APwDGO+cOASkEhnUkFAPPgJRMTaYmIr4LpfAdMAK4Lvg8A0gLcfsOeN7MlpnZrA7ki35JKXDKOYFxfOf8TiMicSyUwr8XmAR8Ofh8P3BPiNs/wzk3FvgUcLWZnXX8CmY2y8xKzay0oqIixM1GmbwS2L8NPn7H7yQiEsdCKfzTnXNXA0cAnHN7CAzrtMk591Hw4w7gSaDJJafOuQXOuSLnXFFubm7IwaPKqecBprN1RMRXoRR+jZklEhiewcxyCeGPtmaWEfwDL2aWQeC0ztUnkDV6ZfaCfuM0ji8ivgql8O8mcHTey8xuB5YAd4bwvt7AEjNbCbwN/N05F7+Nl1cSmEjtwA6/k4hInGrzPHzn3EIzWwZMJ3BO/medc2UhvO99oODEI8aIvGJ45TbY8AKMudTvNCISh0KZD//Pzrl3nXP3OOfmO+fKzOzPnREupvQZBV37alhHRHwTypDOyMZPguP547yJE8PMAkf5G1+G2mq/04hIHGqx8M3sBjPbD4w2s33Bx35gB/BUpyWMJXklUH0ANv/L7yQiEodaLHzn3J3Oua7Az51z3YKPrs65Hs65G46uZ2YjW9qGHGfwWZCUptMzRcQXbQ7pNC73Fmg8P1QpXWDw2bD+H7rqVkQ6XShj+G0JaeZMCcorhj3lsHOD30lEJM6Eo/B1qNoeecWBjzpbR0Q6WTgKX9ojqz/0HqVxfBHpdOEofJ1j2F55xfDBm3B4j99JRCSOhHLhlZnZV83s1uDzAWbWMAmac26ilwFjUl4JuDp47yW/k4hIHPF6emRpTr+x0KWnhnVEpFN5Oj2ytCAhMTBl8nsvQF2t32lEJE54Nj2ytCGvODCGv2Wp30lEJE50dHrkOzxNFQ9OOQcSknR6poh0mlCutF0IXE9gDvxtBKZHftzrYDEvLQsGTtY4voh0mlDO0jkF2OScu4fAHavOM7Nsz5PFg7wSqCgLXHkrIuKxUIZ0ngDqzGwocD8wGHjI01TxIq8k8HH98/7mEJG4EErh1zvnaoHPA79yzs0BTgp1B2aWaGb/MbNnOhoyZvU4BXoM1Ti+iHSKUM/S+TLwdeBoaSe3Yx//A7R5S8S4lVcC5a9D1QG/k4hIjAul8C8ncOHV7c65TWY2GPhLKBs3s/7A+QSGgqQ5ecVQVw3vL/Y7iYjEuFDO0lnrnLvOOfdw8Pkm59xdIW7/lwTO8NF5+y0ZMAlSu2lYR0Q8F8pZOhcEx+B3H73NoZntC+V9wA7n3LI21ptlZqVmVlpRUdGO6DEiMRmGTocNz0O9fi+KiHdCGdL5JXAZ0KPRbQ67hfC+M4BPm1k58Ahwjpk1GQpyzi1wzhU554pyc3Pbkz125JXAge2wbYXfSUQkhoVS+B8Cq51r3z35nHM3OOf6O+cGAZcALzvnvtqBjLFv6HmA6SIsEfFUUgjrXA88a2avAlVHFzrn/s+zVPEmowecPCEwjj+trVsIi4h0TChH+LcDh4A0oGujR8icc4udcxe0P14cySsODOns2+Z3EhGJUaEc4Xd3zs3wPEm8yyuBl34S+OPtuMv8TiMiMSiUI/wXzUyF77VeIyDrZI3ji4hnWi18MzMCY/jPmdnh9pyWKe1kFhjWef8VqDnidxoRiUGtFn7wzJwVzrkE51x6O0/LlPbKK4GaQ1C+xO8kIhKDQhnSedPMxnueRGDQmZDcRVfdiognQin8acC/zWyjmb1jZqvM7B2vg8Wl5DQYMjUwjt++yx5ERNoUylk6n/I8hXwirxjWPQs7yqD3CL/TiEgMabPwnXObOyOIBJ0aPCFq/XMqfBEJq1CGdKQzdesLJxXo9EwRCTsVfiTKK4Etb8Oh3X4nEZEYosKPRHnF4OrhvRf9TiIiMUSFH4lOGgMZvXR6poiElQo/EiUkQN6MwBF+XY3faUQkRqjwI1VeCRyphA/f8juJiMQIFX6kGjIVElM0rCMiYaPCj1SpXWHQFJ2eKSJho8KPZHklsHM97NrodxIRiQGeFb6ZpZnZ22a20szWmNmPvdpXzDp61e2G5/3NISIxwcsj/CrgHOdcAVAIlJjZRA/3F3u6D4bcYRrHF5Gw8KzwXcCB4NPk4ENTQLZXXjGU/wuO6J4zInJiPB3DN7NEM1sB7ABecM7pHMP2yiuB+prAnbBERE6Ap4XvnKtzzhUC/YEJZpZ//DpmNsvMSs2stKKiwss40an/BEjL1tk6InLCOuUsHefcXmAxUNLMawucc0XOuaLc3NzOiBNdEpNg6LmBwq+v9zuNiEQxL8/SyTWz7ODn6cC5wLte7S+m5ZXAoZ3w0XK/k4hIFPPyCP8k4JXg7RCXEhjDf8bD/cWuodPBEnS2joickFBucdghzrl3gDFebT+udOkOJ08MFP45N/udRkSilK60jRZ5xfDxKqjc6ncSEYlSKvxokRf8e/cGna0jIh2jwo8WuadB9kCdnikiHabCjxZmgaP89xdD9SG/04hIFFLhR5O8Yqg9AuWv+51ERKKQCj+aDJoCyRk6PVNEOkSFH02SUuGUaYFxfKd56ESkfVT40SavBPZthe2r/U4iIlFGhR9tjt4URcM6ItJOKvxo07U39B2r0zNFpN1U+NEorwS2lMIBTSctIqFT4UejvGLAwXsv+J1ERKKICj8anVQAXU/SOL6ItIsKPxqZBf54+97LUFvtdxoRiRIq/GiVVwLV++GDN/xOIiJRQoUfrYacDYmpOltHREKmwo9WKRkw+CxY9w9ddSsiIfHynrYnm9krZlZmZmvM7H+82lfcyiuGPZtg13t+JxGRKODlEX4t8B3n3HBgInC1mY3wcH/xJ6848FHDOiISAs8K3zm3zTm3PPj5fqAM6OfV/uJS9gDoNVKnZ4pISDplDN/MBhG4oflbnbG/uJJXDB+8CYf3+p1ERCKc54VvZpnAE8C3nXP7mnl9lpmVmllpRYWmCmi3vBKor4WNL/udREQinKeFb2bJBMp+oXPur82t45xb4Jwrcs4V5ebmehknNvUvgvTuGscXkTZ5eZaOAb8Hypxz/+fVfuJeQmLgqtsNz0N9nd9pRCSCeXmEfwbwNeAcM1sRfPyXh/uLX3nFcHh3YAZNEZEWJHm1YefcEsC82r40cso5kJAUOFtnwOl+pxGRCKUrbWNBejYMmKRxfBFplQo/VuSVwI41sPcDv5OISIRS4ceKvJLARx3li0gLVPixoudQ6H6KCl9EWqTCjyV5JbDpNag+6HcSEYlAKvxYklcMdVXw/qt+JxGRCKTCjyUDJkFKV02mJiLNUuHHkqQUGHpOYBxfN0URkeOo8GNNXgkc+Bi2rfQ7iYhEGBV+rBl6HmDw8m2w412/04hIBFHhx5rMXJh2I5S/DveeDgsvDpy5oyEekbinwo9FZ18Pc9bA1Bth6zJ48EJYcDa88zjU1fidTkR8osKPVRk9Yer3Yc5quPBXUHMY/vpN+FUhvDEfjjS5F42IxDgVfqxLTodxM2H2W/DlRyFnEDx/E8wbCc/fDJVb/E4oIp1EhR8vEhLgtBK4/O9w5Stw6nnw5r3wqwJ44kqd1SMSB8xF0B/zioqKXGmpbuLRafZ+AP++D5Y/CNUHYPBZMOlaGHpu4BeEnLj6eup3b2LH+rc4tH0jkfTzJpElZ9wX6T5wRIfea2bLnHNFba3n2Q1QgiH+AFwA7HDO5Xu5L+mA7AFQckfgj7zLHwyU/0MXQe4wmHQ1jLoYktP8Thk96mqp3vEuO9a9zcHNy0itWE3uwfVkuEP08TubRLz/ZA7pcOGHytMjfDM7CzgA/CmUwtcRvs9qq2HNk/DGr2H7KsjoBRNmwfhvQJfufqeLLLXVHNq6mu3r3uLIB8vpsmsNvQ+/RxpVABx2KaxjIB9nnEZVz1GkDxxDzwEjSUjy9BhLotig3CyyM9M79N5Qj/A9H9Ixs0HAMyr8KOIcbHo1UPzvvQhJ6TDmqzBpNnQf4ne6zld9iL3l/2HHhqXUbvkPmbvXcFLVJpKpBWC/S2e9DaYi8zRqeo0mc/A4BuYVMCg3i4QE3eVTvBcRQzoSpcxgyNTAY/taePMeWPZHWHo/DL8gMM4fo/fOdUcqqdhQyq73luI+WkHW3jL61HxANvVkA3tcJhsST2Ft9hdxfUaTNaSIIXmjGJuVjpnKXSKb70f4ZjYLmAUwYMCAcZs3b/Y0j3TQ/o/h7QWw9PdwZC/0nwCTr4Vh50NCot/pOqR2/04+Xv82ezcuxba9Q/d9ZZxUt7Xh9e0um03JQ6nMHkFC30Jyho7n1FOGkZWR4mNqkaY0pCPeqDoAKxYGjvr3boacwYE/8BZ+BVIy/E7XoiO7t/JR2b85UL6MpO3v0PPAu/Sqr2h4fYvL5YPUU9mfM5Lk/oXknjqeoUOGkp4Snb/MJL6o8MVb9XVQ9jS8OR+2LIX0HCj6RuCPvF17e7jfeqiqpPbgHg7sreBg5S6O7NtJ9f7d1B7chTu8Fw7vIbG6kuTqSlJr9tGtbjc5rrJhE5s4iY/S8zjcI5/Uk8fS57QJDD65P0mJOhVVolNEFL6ZPQxMBXoC24EfOud+39L6Kvwo9cFb8Mbd8O7fITEZRl8Mk66BXsObX9+5wFQPh/dQf2gPBysrOLxvF4f37aLmwG7qDu6m/vAe7MhekqoqSa6pJK12P10E6lRiAAAH+ElEQVTq9pHhDpJAy9+zh10Ke8nkoGVyMLErVUndqErJpqbnCDIGjKHfsAn069NL4+0SUyKi8NtLhR+dqmvrOVhVy5Ht60kt/S1Z6x4jse4IO3qdwf6k7iQc2UtiVSUpNZWk1e0no25fwxkuzal1CVSSwV6XycGETA4Fi7s6OYv61Cxceg6Wnk1SRndSunYnrVtPMrJ6kJmVS3ZWV7qmJqnQJa7oLB1pVX2942B1LQeqajlwJPgx+Pn+qloONlre+Pn+4MeD1Z88r66tb7TlEnKYzKWJL3Hxx4tJNcdel8kBy+RQYl+qk7tR0yWbutQsSM8mIT2HxIzupHbtQZesHqR360lWVg7ZGakMSk8mUac1ioSNCt9nzjmq6+qpqXNU19ZTU1dPdW19cFl9o2XB9Y577dhlrmHZ4eo69h+p5UBVDQer6oJFXcOBqloOVtVxoKrlI+zGUpMS6JqWREZqEpnBx0lZaWSmBZ+nJZGZkvTJ86PLUkuoSk0iq0syeekppCRpfFzEbzFR+E+t2MridRXHzFNy9LPGI1afLGu6Hses55q+1x372rHLPlFX76ipq6cqWNSflLZrtshr6sI/pJacaHRJObaAs9OT6Z+T3kI5H7cs+MhITVJRi8SQmCj8LXsOs2zzHiBwzdBRRz9tPJ5rTT5pfb1jt2dNljW8FlyYmAApiQkkJyaQmZrU8HlK0icfUxKtmWWfPE9OtGaWHb9eYBupScduPznRNH4tIs2KicK/etpQrp421O8YIiIRTf9fFxGJEyp8EZE4ocIXEYkTKnwRkTihwhcRiRMqfBGROKHCFxGJEyp8EZE4EVGzZZpZJbCh0aIsoPK4z49fRvB5T2BnO3bXeDttLT9+WSi5ji5LVi7lakeOlnI1l1G52p+rvV0RLbkGOudy23yXcy5iHsCClp4f/fz4ZY2Wl57Ivlpb3pFcjfIpl3KFnKOlDM1lVK725wp+DDlbNOZq7RFpQzpPt/L86RaWHf+eju6rteUdydXaPpQrfnO1laOlDM3lUS7lapeIGtI5EWZW6kK4AUBnU672Ua72Ua72i9RsnZEr0o7wT8QCvwO0QLnaR7naR7naL1KzeZ4rZo7wRUSkdbF0hC8iIq1Q4YuIxAkVvohInIibwjezDDNbZmYX+J3lKDMbbmb3mdkiM/uW33mOMrPPmtnvzOwpM5vhd56jzGyImf3ezBZFQJYMM3sw+HW61O88R0XS16ixCP6eisifQfCoszpy8n5nPoA/ADuA1cctLwHWAe8BPwhhOz8Bvg9cEEm5gu9JAH4fgblyIjTXIr+/14CvARcGP3/Uizwn8rXz6msUhlxh+54Kc66w/QyGK1e4O8s5FxWFfxYwtvEXCUgENgJDgBRgJTACGAU8c9yjF3AucAkwM4yFf8K5gu/5NPAG8JVIyhV83y+AsRGYy6vCb0/GG4DC4DoPRcrPgNdfozDkCtv3VLhyhftnMEzfX2HvLOdc5N/E3Dn3mpkNOm7xBOA959z7AGb2CPAZ59ydQJP//pjZNCCDwBfysJk965yr9ztXcDt/A/5mZn8HHjqRTOHKZWYG3AX8wzm3/EQzhSuX19qTEdgC9AdW4PHQaDtzrfUyS0dzmVkZYf6eCkcuYG24fwbDlCuTMHcWEPmF34J+wIeNnm8BTm9pZefcTQBmNhPYGY4vXDhymdlU4PNAKvCsR5nanQu4lsARRpaZDXXO3RcJucysB3A7MMbMbgj+YvBaSxnvBuab2fl0fHqPsOfy6WvUZi4673uqXbk68WewXbmcc9dA+DsrWgvfmlnW5hVkzrk/hj/KMdqVyzm3GFjsVZhG2pvrbgKF5rX25toFXOVdnGY1m9E5dxC4vJOzNNZSLj++Ro21lKuzvqda0lKuxXTOz2BLWv0ZCHdnRetZOluAkxs97w985FOWxpSrfSI1V2ORmlG52ke5iN7CXwqcamaDzSyFwB83/uZzJlCu9orUXI1Fakblah/lgqg4S+dhYBtQQ+C34TeCy/8LWE/gL9w3KZdyxWpG5VKucD00eZqISJyI1iEdERFpJxW+iEicUOGLiMQJFb6ISJxQ4YuIxAkVvohInFDhS8wzswNh2s6PzOy7Iaz3RzP7Yjj2KRJOKnwRkTihwpe4YWaZZvaSmS03s1Vm9png8kFm9q6Z3W9mq81soZmda2b/MrMNZjah0WYKzOzl4PIrg+83M5tvZmuDU+z2arTPW81saXC7C4JTT4v4QoUv8eQI8Dnn3FhgGvCLRgU8FPgVMBoYBnwFmAJ8F7ix0TZGA+cDk4Bbzawv8DngNAI3brkSmNxo/fnOufHOuXwgHR/m+Rc5KlqnRxbpCAPuMLOzgHoCc5H3Dr62yTm3CsDM1gAvOeecma0CBjXaxlPOucMEbkrxCoEbWJwFPOycqwM+MrOXG60/zcyuB7oA3YE1+DN/vogKX+LKpUAuMM45V2Nm5UBa8LWqRuvVN3pez7E/J8dPPuVaWI6ZpQH3AkXOuQ/N7EeN9ifS6TSkI/EkC9gRLPtpwMAObOMzZpYWvLPUVALT274GXGJmiWZ2EoHhIvik3HeaWSagM3fEVzrCl3iyEHjazEoJ3Iv23Q5s423g78AA4KfOuY/M7EngHGAVgWluXwVwzu01s98Fl5cT+OUg4htNjywiEic0pCMiEidU+CIicUKFLyISJ1T4IiJxQoUvIhInVPgiInFChS8iEidU+CIiceL/A7D6+KteqCeFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "perform_plot = sns.lineplot('lambda','rmse_train',data=hyperparam_perform)\n",
    "perform_plot.set(xscale='log')\n",
    "sns.lineplot('lambda','rmse_test',data=hyperparam_perform,ax=perform_plot)\n",
    "plt.legend(labels=['RMSE training data', 'RMSE test data'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
